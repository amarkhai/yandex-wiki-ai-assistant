#!/usr/bin/env python3
import json
import os
import sys
from pathlib import Path
from typing import List, Tuple

import faiss
import numpy as np
from dotenv import load_dotenv
from openai import OpenAI

# --- I/O encoding guards -----------------------------------------------------
# Try to force stdin/stdout to UTF-8 and ignore invalid bytes from the terminal
try:
    sys.stdin.reconfigure(encoding="utf-8", errors="ignore")
    sys.stdout.reconfigure(encoding="utf-8", errors="ignore")
except Exception:
    pass

VECTOR_DIR = Path(os.getenv("VECTOR_DIR", "vector_store"))
INDEX_PATH = VECTOR_DIR / "index.faiss"
META_PATH = VECTOR_DIR / "meta.jsonl"
MANIFEST_PATH = VECTOR_DIR / "manifest.json"

# Response language (default Russian as requested)
RAG_RESPONSE_LANG = os.getenv("RAG_RESPONSE_LANG", "ru").lower()

# ‚Äî‚Äî Text hygiene ‚Äî‚Äî

def scrub(text: str) -> str:
    if text is None:
        return ""
    return text.encode("utf-8", errors="ignore").decode("utf-8", errors="ignore")


def safe_input(prompt: str) -> str:
    """Robust input that survives bad terminal encodings."""
    try:
        return input(prompt)
    except UnicodeDecodeError:
        # Fallback: read raw bytes and decode leniently
        sys.stdout.write(prompt)
        sys.stdout.flush()
        data = sys.stdin.buffer.readline()
        return data.decode("utf-8", errors="ignore")

# ‚Äî‚Äî Retrieval helpers ‚Äî‚Äî

def load_index_and_meta():
    if not INDEX_PATH.exists() or not META_PATH.exists():
        raise SystemExit("Vector store not found. Run ingest_md.py first.")
    index = faiss.read_index(str(INDEX_PATH))
    meta = []
    with META_PATH.open("r", encoding="utf-8") as f:
        for line in f:
            meta.append(json.loads(line))
    manifest = {}
    if MANIFEST_PATH.exists():
        try:
            manifest = json.loads(MANIFEST_PATH.read_text(encoding="utf-8"))
        except Exception:
            manifest = {}
    return index, meta, manifest


def search(index, meta, query_vector: np.ndarray, k: int = 5) -> List[Tuple[float, dict]]:
    q = query_vector.astype("float32")[None, :]
    faiss.normalize_L2(q)
    scores, ids = index.search(q, k)
    out = []
    for score, idx in zip(scores[0].tolist(), ids[0].tolist()):
        if idx == -1:
            continue
        out.append((float(score), meta[idx]))
    return out


# ‚Äî‚Äî Embedding selection (symmetry with ingest) ‚Äî‚Äî
try:
    from sentence_transformers import SentenceTransformer
    _has_st = True
except Exception:
    _has_st = False

_embedder_cache = None

def get_query_embedder(manifest: dict):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç callable(text)->np.ndarray, —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Å –º–æ–¥–µ–ª—å—é –∏–Ω–∂–µ—Å—Ç–∞.
    –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:
      1) RAG_EMBED_MODEL_OVERRIDE (—Ç–æ—á–Ω–æ–µ –∏–º—è HF –∏–ª–∏ 'openai:<model>')
      2) manifest['embedding_model']
      3) –¥–µ—Ñ–æ–ª—Ç—ã
    """
    embed_id = manifest.get("embedding_model", "sentence-transformers/all-MiniLM-L6-v2")
    override_name = os.getenv("RAG_EMBED_MODEL_OVERRIDE", "").strip() or embed_id
    override_mode = os.getenv("RAG_QUERY_EMBEDDER", "").lower().strip()  # 'st' | 'openai'

    def st_embedder_factory(model_name: str):
        def _f(text: str) -> np.ndarray:
            nonlocal model_name
            global _embedder_cache
            if _embedder_cache is None:
                from sentence_transformers import SentenceTransformer
                _embedder_cache = SentenceTransformer(model_name)
            # e5-–º–æ–¥–µ–ª–∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é—Ç –ø—Ä–µ—Ñ–∏–∫—Å—ã
            qtext = f"query: {text}" if "e5" in model_name.lower() else text
            vec = _embedder_cache.encode([qtext], convert_to_numpy=True, normalize_embeddings=True)[0]
            return vec.astype("float32")
        return _f

    def openai_embedder_factory(model_name: str):
        def _f(text: str) -> np.ndarray:
            client = OpenAI()
            emb = client.embeddings.create(model=model_name, input=text)
            return np.array(emb.data[0].embedding, dtype="float32")
        return _f

    # –Ø–≤–Ω—ã–π –æ–≤–µ—Ä—Ä–∞–π–¥ —Ä–µ–∂–∏–º–æ–º
    if override_mode == "openai" and os.getenv("OPENAI_API_KEY"):
        model_name = override_name.split(":", 1)[1] if override_name.startswith("openai:") else "text-embedding-3-small"
        return openai_embedder_factory(model_name)
    if override_mode == "st":
        return st_embedder_factory(override_name)

    # –ê–≤—Ç–æ-—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –ø–æ –∏–º–µ–Ω–∏
    if override_name.startswith("openai:") and os.getenv("OPENAI_API_KEY"):
        return openai_embedder_factory(override_name.split(":", 1)[1])

    # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚Äî SentenceTransformers –¥–ª—è –ª—é–±–æ–≥–æ HF id
    return st_embedder_factory(override_name)

# ‚Äî‚Äî LLM call ‚Äî‚Äî

def call_llm(question: str, context_blocks: List[str]) -> str:
    load_dotenv()
    model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
    question = scrub(question)
    context_blocks = [scrub(c) for c in context_blocks]

    if not os.getenv("OPENAI_API_KEY"):
        joined = "\n\n".join(context_blocks) if context_blocks else "(–∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω)"
        return f"[DEV MODE / RU]\n–í–æ–ø—Ä–æ—Å: {question}\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{joined}\n\n–û—Ç–≤–µ—Ç (–∑–∞–≥–ª—É—à–∫–∞): –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ OPENAI_API_KEY –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤."

    client = OpenAI()
    system = (
        "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏. –û—Ç–≤–µ—á–∞–π –Ω–∞ –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï. "
        "–û–ø–∏—Ä–∞–π—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–µ CONTEXT-—Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã; –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, —Ç–∞–∫ –∏ —Å–∫–∞–∂–∏ –∏ –ø—Ä–µ–¥–ª–æ–∂–∏, –≥–¥–µ –µ–≥–æ –∏—Å–∫–∞—Ç—å. "
        "–î–æ–±–∞–≤–ª—è–π –∫—Ä–∞—Ç–∫–∏–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ (source: filename.md#chunk)."
    )
    if RAG_RESPONSE_LANG.startswith("ru"):
        user_prefix = "–í–æ–ø—Ä–æ—Å"
        ctx_label = "–ö–û–ù–¢–ï–ö–°–¢"
    else:
        user_prefix = "Question"
        ctx_label = "CONTEXT"

    context_text = "\n\n".join(context_blocks)

    messages = [
        {"role": "system", "content": system},
        {"role": "user", "content": f"{user_prefix}: {question}\n\n{ctx_label}:\n{context_text}"},
    ]

    resp = client.chat.completions.create(model=model, messages=messages, temperature=0.2)
    return resp.choices[0].message.content.strip()



## Modified format_citations to generate wiki links
def format_citations(hits: List[Tuple[float, dict]]) -> List[str]:
    """–§–æ—Ä–º–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ wiki.yandex.ru –∏–∑ –ø—É—Ç–µ–π doc_path."""
    cites = []
    cwd = os.getcwd()
    for score, m in hits:
        raw_path = Path(m["doc_path"]).as_posix()

        # 1. –£–¥–∞–ª—è–µ–º —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        if raw_path.startswith(cwd):
            raw_path = raw_path[len(cwd):]

        # 2. –£–¥–∞–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å /var/out/yandex-wiki-catalog
        raw_path = raw_path.replace("/var/out/yandex-wiki-catalog", "")

        # 3. –£–¥–∞–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ .md
        if raw_path.endswith(".md"):
            raw_path = raw_path[:-3]

        # 4. –£–¥–∞–ª—è–µ–º _index, –µ—Å–ª–∏ —ç—Ç–æ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç –ø—É—Ç–∏
        raw_path = raw_path.replace("/_index", "")

        # 5. –£–¥–∞–ª—è–µ–º –≤–µ–¥—É—â–∏–µ —Å–ª—ç—à–∏
        raw_path = raw_path.lstrip("/")

        # 6. –°–∫–ª–µ–∏–≤–∞–µ–º –±–∞–∑–æ–≤—ã–π URL
        wiki_url = f"https://wiki.yandex.ru/{raw_path}"

        cites.append(f"[{wiki_url}]( {wiki_url} ) (score={score:.3f})")

    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
    seen = set()
    uniq = []
    for c in cites:
        if c not in seen:
            seen.add(c)
            uniq.append(c)
    return uniq

def main():
    load_dotenv()
    index, meta, manifest = load_index_and_meta()
    embedder = get_query_embedder(manifest)

    print("\nüîç Markdown RAG Agent ‚Äî –≤–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å (–∏–ª–∏ 'exit')\n")
    while True:
        try:
            q = safe_input("–í—ã: ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\n–ü–æ–∫–∞!")
            break
        if not q:
            continue
        if q.lower() in {"exit", "quit", ":q", "–≤—ã—Ö–æ–¥"}:
            print("–ü–æ–∫–∞!")
            break

        q = scrub(q)
        qvec = embedder(q)

        if qvec.shape[0] != index.d:
            print(f"[WARN] –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞ {qvec.shape[0]} != —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –∏–Ω–¥–µ–∫—Å–∞ {index.d}. –ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä—É–π—Ç–µ –∏–ª–∏ –∑–∞–¥–∞–π—Ç–µ RAG_QUERY_EMBEDDER.")
            continue

        hits = search(index, meta, qvec, k=6)
        blocks = [h[1]["text"] for h in hits]
        citations = format_citations(hits)

        answer = call_llm(q, blocks)
        print("\n–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:\n" + answer + "\n")
        if citations:
            print("–ò—Å—Ç–æ—á–Ω–∏–∫–∏:")
            for c in citations:
                print(" - ", c)
        print()


if __name__ == "__main__":
    main()
